<article>
  <front>
    <journal-meta />
    <article-meta>
      <title-group>
        <article-title>Geo-based Automatic Image Annotation</article-title>
      </title-group>
      <contrib-group>
        <aff id="0">
          <label>0</label>
          <institution>Algorithms</institution>
          ,
          <addr-line>Experimentation, Performance</addr-line>
        </aff>
        <aff id="1">
          <label>1</label>
          <institution>Harald Kosch University of Passau Innstrasse 43 94032 Passau</institution>
          ,
          <country country="DE">Germany</country>
        </aff>
        <aff id="2">
          <label>2</label>
          <institution>Gabriele Gianini University of Milan via Bramante</institution>
          ,
          <addr-line>65 26013 Crema</addr-line>
          ,
          <country country="IT">Italy</country>
        </aff>
        <aff id="3">
          <label>3</label>
          <institution>Image Annotation</institution>
          ,
          <addr-line>Geotagging, Image Retrieval, Statistical Models</addr-line>
        </aff>
      </contrib-group>
      <abstract>
        <p>A huge number of user-tagged images are daily uploaded to the web. Recently, a growing number of those images are also geotagged. These provide new opportunities for solutions to automatically tag images so that e cient image management and retrieval can be achieved. In this paper an automatic image annotation approach is proposed. It is based on a statistical model that combines two di erent kinds of information: high level information represented by user tags of images captured in the same location as a new unlabeled image (input image); and low level information represented by the visual similarity between the input image and the collection of geographically similar images. To maximize the number of images that are visually similar to the input image, an iterative visual matching approach is proposed and evaluated. The results show that a signi cant recall improvement can be achieved with an increasing number of iterations. The quality of the recommended tags has also been evaluated and an overall good performance has been observed.</p>
      </abstract>
    </article-meta>
  </front>
  <body>
    <sec id="1">
      <title>-</title>
      <p>Categories and Subject Descriptors
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.</p>
      <p>ICMR â€™12, June 5-8, Hong Kong, China
Copyright c 2012 ACM 978-1-4503-1329-2/12/06 ...$10.00.</p>
      <sec id="1-1">
        <title>1. INTRODUCTION</title>
        <p>Recent technological development has allowed an easy
access to digital photography devices, such as digital
cameras and smart phones. Everyday, individuals are producing
large amounts of images and upload them to the web. For
example, Flickr1 hosts about 6 billion images. As a result,
image organization and retrieval became more challenging
and e cient solutions are required.</p>
        <p>Research in content based image retrieval (CBIR) [6] [5]
addresses the problem by identifying, combining and
comparing di erent kinds of low level image features, such as
color histogram, texture, etc. Although this alleviates the
problem, CBIR alone is still unable to cope with the
semantic notions of user queries. In fact, most CBIR systems
follow a query by example (QBE) scheme which is still far
too less spread than the conventional keyword-based image
search.</p>
        <p>To narrow the semantic gap, annotating images with
keywords, known as tags is widely used. Tagging enables
easier management and retrieval of images. Recently, in
addition to the huge amount of user tagged images which are
available on the web, the number of images which are
provided with location information "geotagged" is increasing.
Geotagged images contain in their Exif descriptors [16] the
coordinates (latitude and longitude) of the location of their
capture. Currently, Flickr2 hosts more than 171 million
geotagged items, while a statistic of the year 2007 shows that
Panoramio3 hosts 2 million geotagged images.</p>
        <p>The high availability of user- and geotagged images
provides opportunities to develop new tools for automatic image
annotation. This can be done by combining the high level
information represented by user tags and CBIR solutions
which work on image low level features. For better
understanding, consider the following scenario.</p>
        <p>Assume that a user took a photo of the building of Institute
de France (Fig. 4a) with his smart phone. The GPS
receiver of the phone calculates the latitude and the longitude
of the location and the data are stored in the Exif descriptor
of the produced image. Later on, the user wants to upload
this image to his favorite social website but he doesn't have
time to tag it, or he doesn't even know about the place and
the contents of the image. The user can send the image
to a tag recommendation system which extracts the GPS
coordinates of the image. Then, the system applies a
geobased search on online image databases, such as Flickr to
nd user-tagged images which were taken by other users in
the surroundings of Institute de France. From the retrieved
set, the system tries to nd images in which the building
of Institute de France appears. For this purpose, the
system searches for images from the retrieved collection which
are visually similar to the input image. Finally, the tags of
the visually matching images can be analyzed to produce
annotation suggestions for the input image.</p>
        <p>To address each of the steps presented in the above
image annotation scenario, a statistical model for automatic
image annotation is proposed. It combines high level (user
tags) and low level information extracted from community
image to annotate a new image taken in the same location.
To maximize the size of the candidate tag collection an
iterative image matching approach is provided. It increases
the number of visually similar images and, as a result, the
number of the collected tags. This has a direct in uence
on the quality of the provided tags. First, additional
candidate tags can be discovered. Second, form statistical point
of view, the usage pattern of user tags can be better
estimated in a larger tag collection. To reduce the runtime of
the visual matching, an image clustering approach based on
image low level feature similarity is introduced. Finally,
different evaluation studies for estimating the e ectiveness of
the introduced solutions are provided.</p>
        <p>The rest of the paper is organized as follows: in the next
section related work is reviewed. In section 3 a detailed
description of the annotation approach is presented. In section
4 di erent evaluation scenarios are provided. The paper is
concluded and future work is discussed in section 5.</p>
        <p>RELATED WORK</p>
        <p>This work is related to search-based image annotation
approaches. It exploits geo- and user-tagged image datasets
and proposes a probabilistic model for tag recommendation.
This model combines low level image features and user tags
to produce word-image relevance values which can be used
to rank the recommended tags. In the following, we review
image annotation works which use similar techniques.</p>
        <p>The authors in [18] introduce a hybrid probabilistic model
for automatic image tagging. The model combines CPAM
(Colored Pattern Appearance Model) [15] image features
with textual information provided by the user as initial tags.
The goal is to extend the set of provided initial tags by
recommending new ones.</p>
        <p>In [11] user-tagged community photos are exploited to
provide a tag ranking approach. Hereby, the importance of the
tag is determined by the number of visual neighbors of the
input image which are annotated with that tag. The
visual neighbors are identi ed based on three image features,
namely, color correlgoram, color texture moment and RGB
color moment.</p>
        <p>The tag recommendation approach presented in [17]
generates tag rankings by combining three kinds of tag
correlations is presented in: tag co-occurrence in a large Flickr
image dataset, tag correlation calculated based on visual
language model (VLM), and image-tag conditioned tag
correlation.</p>
        <p>The image annotation approach proposed in [12] combines
di erent search schemes to estimate the relation between a
word and an unlabeled image. For this purpose,
keywordbased image retrieval is used to calculate the likelihood of
obtaining a certain image given a certain keyword. After
that, the set of candidate annotations is expanded by
applying a linear combination of two kinds of search-based word
correlations. A statistical correlation that uses the words
as input to Google image searcher and calculates
"Normalized Google Distance" [4] based on the provided results. The
other correlation is based on the visual consistence among
the resulted images returned by the search.</p>
        <p>A geo-based image annotation approach for landmark
photos is proposed in [7]. It uses multi-level clustering approach
that exploits geographical as well as visual features to
identify the category of an input image and the nearest visual
clusters. A caption for the input image is generated form
the set of most frequently used tags in the corresponding
visual cluster.</p>
        <p>Among the above introduced works only [7] consider
geotagged community photos. In fact, our approach share
similar ideas, however, we have a broader focus than only
providing captions for landmark photos. We provide a solid
framework for tag recommendation that exploits visual,
geographical and di erent tag occurrence measures to provide
ranked tag suggestions, while image captions produced in [7]
depend only on the usage frequency of the tags in the visual
cluster.</p>
        <p>The statistical model provided here is similar to the one
proposed by [18]. However, our approach considers the
geographical context of the input image and uses a di erent
family of low level image features. Furthermore, we don't
expect any further input for our approach except the input
image, while initial tags are considered as perquisite to
generate tag recommendations in [17]. In [18] the absence if
initial tags leads to a pure CBIR search.</p>
        <p>Finally, no one of the approaches presented in this section
considers the e ect of the volume of the image dataset on the
quality of the suggested tags. In this paper we deal with this
problem and propose an iterative visual matching solution
to nd additional visual correspondences for an input image.
We also show how the size of the visual cluster in uences the
quality of the produced tags.</p>
        <p>TAG RECOMMENDATION APPROACH
The proposed approach consists of three phases (see Fig.1).
A geo-based search in which the geographical coordinates
of an input image are used to nd a collection of
usercontributed images that are taken in the same geographical
area. Hereby, the web image databases Flickr and Panoramio
are used since they contain a large amount of geo- and
usertagged images and provide a freely available API. Next, a
visual matching is applied on the retrieved image collection
to identify images that are visually similar to the input
image. Finally, user-tags of the visual correspondences are
analyzed and used to generate tag recommendations for the
input image. The output of each phase is integrated into a
probabilistic model that generates scores (ranks)
representing how suitable is a candidate tag for the input image.</p>
      </sec>
    </sec>
    <sec id="2">
      <title>Problem Statement</title>
      <p>Let Iu be an unlabeled image taken at a location (lt; ln)
where lt and ln represent the latitude and the longitude of
the location respectively. Let C = fI1; I2; :::; Img be a set
of images taken in the same geographical area as Iu, such
that 8Ii 2 C ; distance(Iu; Ii) &lt; d, where distance(Iu; Ii) is
the geographical distance between the locations of capture
of the two images and d is a prede ned distance threshold.
Furthermore, suppose that members of C are annotated with
words taken from the lexicon W = fw1; w2; :::; wng. Each
word in W can be used at most for one time to annotate
the same image, however, the same word can be used to
annotate more than one image.</p>
      <p>The introduced components and their relationships can be
visualized by an undirected graph G(V; E) where the vertices
corresponds to elements from V = W [ C (the union of word
and image sets). Edges connect words and images and are
de ned as E = f(w; I)jw 2 W ^ I 2 Cg. An edge indicates
that a word has been used to annotate the linked image. The
problem of recommending a tag to an input image can be
expressed by nding edges between the vertex representing
the input image and the set of vertices which correspond to
words. Such edges are represented as dashed lines in Fig.</p>
      <sec id="2-1">
        <title>2. To achieve that, an edge can be drawn between Iu and a</title>
        <p>word w if there is an image Ii 2 C which is visually identical
to Iu (i.e. a copy of Iu) and annotated with w. However,
this assumption is too strict and to loosen it, we assume
that an edge can be drawn between Iu and a word w if
some kind of similarity relationship exists between Iu and
an image Ii which is annotated with w. There are di erent
possibilities to derive such a relationship: in this paper, we
will use Bayesian methods to derive a relationship from the
visual similarity between the images Iu and Ii.</p>
      </sec>
    </sec>
    <sec id="3">
      <title>Pseudo-generative Statistical Model</title>
      <p>We frame the problem within a probabilistic model based
on Bayes' Rule (a.k.a. Total Probability Law).
Paradigmatically the Bayes' Rule is used in settings where, from a
generative model{ linking some causes (endowed with an
apriori probability) to some e ects { one wants to get the
probability of the e ects given a cause. This generative
model is typically represented by means of a tree, where
the leaves correspond to the e ects, (or by means of a more
compact graph where the e ects can be shared among
different causes). Complying to this paradigm, we can identify,
for sake of convenience, the input image Iu with the root of
the tree, and the words wi with the leaves, whereas the
images Ij play the role of intermediate nodes, "caused" by the
root and "causing" the leaves. Within this pseudo-generative
model the input image Iu can be thought to "yield" each of
the images Ij 2 C, with a probability value P (IijIu); in
turn images from C can be thought to "yield" the words wi
{ which annotate them { with probability P (wijIj). This
model in the compact version is shown in the graph of Fig.
3.</p>
      <p>Let us assume that the values of the conditional
probabilities P (IjjIu) and P (wijIj) are available, then we can
de ne the strength of the relationship of Iu with a word wi
to be equal to the conditional probability P (wijIu), which,
given an input image Iu, can be computed by means of the
Bayes'Rule as</p>
      <p>P (wijIu) =
This corresponds to the sum over all the paths leading from
the root image Iu to the leaf word wi, of the path's
probability computed by chain product. Therefore, by construction,
only words that are reachable from the input image are
considered as candidate tags. Once the probabilities P (wijIu)
have been computed for all the candidate tag words relevant
to a speci c input image, the probability values provide a
natural ordering of the importance/appropriateness of the
words to the input image, and few of them, the top ranking
ones, as actual input image tags.</p>
      <p>The assumption of the availability of the values of the
conditional probabilities P (Ij jIu) and P (wijIj ) needs
further consideration: it is a strong assumption, however it can
be easily relaxed. Strictly speaking neither the generative
process from Iu to the Ij 's nor the generative model from
the Ij 's to the wi's are known or de ned precisely, hence
the above conditional probabilities cannot be known exactly.
However we are not interested in probabilities "per-se", but
rather in probability values as indicators used eventually for
ranking the di erent candidate tag words (for
appropriateness with respect to a speci c input image). Therefore even
quantities proportional to (or simply monotonically
dependent on) those probabilities will suite the task, because they
will not change the ordering. Furthermore if the
probability gaps between pairs of images (P (Ij jIu) P (Ij0 jIu) with
Ij ; Ij0 2 C) relevant to an input image are wide enough, then
even slightly distorting functions or indicators correlated
with the P (:jIu) (proxies of the P (:jIu)) can suite the task.
In an analogous way proxies of the P (:jIu) can can be used in
their place if the probability gaps between pairs of candidate
tag words describing a tagged image (P (wijIj ) P (wi0jIj )
with wj ; wj0 2 W (Ii)) are wide enough . For those reasons
even if the conditional probabilities P (Ij jIu) and P (wijIj )
are not directly available to us, we will adopt the above
described ranking procedure: in place of the probabilities we
will use proxy quantities { respectively an image-to-image
similarity measure and a word-to-image importance
measure { which are introduced in the next subsection.</p>
      <sec id="3-1">
        <title>3.2.1 Image-to-Image Similarity</title>
        <p>The term P (IijIu) represents the probability of
generating the image Ii from the input image Iu which was taken in
the same geographical neighborhood. There exist di erent
kinds of information that can be exploited and combined to
calculate this probability. For example, some of this
information can be extracted from user pro le similarities and
other from the geographical context of the image pair, such
as the geographical distance. In this paper, we only consider
image visual similarity based on low-level image features to
estimate this probability.
Images which are taken in a certain location vary according
to the camera perspective, zoom, light conditions, etc.
Correspondingly, nding visually similar images requires
matching the images based on low level features that are robust
against changes in the scale, rotation, skew and illumination.
Recently, various approaches for image matching based on
nding correspondences among distinctive points, called
interest points, have shown a great success in wide range of
applications. The general approach can be described in a
three step process. First, a detector is used to identify
interest points in the image, such as corners or blobs. Next,
every interest point is represented by a distinctive feature
vector known as descriptor. Finally, correspondences are
determined based on the distance (Euclidean distance or
Mahalanobis distance) between the two descriptors. Among
the di erent image matching algorithms that follow this
approach we selected SURF (Speeded Up Robust Features)
[2]. The SURF detector and descriptor are scale and
rotation invariant. A special characteristic of SURF is also that
it uses a small sized descriptor. This allows fast matching
and makes the algorithm suitable for online applications.
To achieve this, SURF uses integral images to reduce the
computation required by a 'Hessian' detector.
Additionally, SURF descriptor is built from Haar-wavelet responses
within the interest point neighborhood and has a size of only
64 dimensions. In [8] it has been shown that SURF
outperforms other algorithms from the same family, namely SIFT
[13] and PCA-SIFT [9]. The evaluation shows that SURF
is faster than the other approaches and provides matching
results comparable to that of SIFT.</p>
        <p>To estimate the term P (IijIu), the visual similarity between
the two images is calculated. For this purpose, the SURF
descriptor of each image is extracted and common
interest points are determined. The similarity of the image pair
can be calculated by computing the Dice's (intersection over
union) similarity coe cient, which compares the number of
common elements with the total elements of two sets. The
Dice's coe cient is de ned as follows:</p>
        <p>Sim(Ii; Ij ) =
where:
- IP s(Ii): The set of interest points extracted from Ii.
- IP s(Ij ): The set of interest points extracted from Ij .
- IP s(Ii) \ IP s(Ij ) The set of common interest points. It
consists of interest point pairs which have minimum
Euclidean distance between their descriptor vectors4.</p>
        <p>Now a proxy of the probability P (IijIu) can be obtained
by normalizing the visual similarity between Ii and Iu
according to the total similarity between Iu and all images
form C.</p>
        <p>P (IijIu) =</p>
      </sec>
    </sec>
    <sec id="4">
      <title>Iterative Image Matching</title>
      <p>Approaches for interest point based image matching fall
short of discovering similarity between two versions of the
same image, for example, if the image is rotated by an
angle exceeding some threshold. In our scenario, this results in
loosing images which are visually similar to the input image,
thus, additional candidate tags can also be missed.</p>
      <p>The recall of image matching that is based on interest
points similarities can be improved by applying the
matching algorithm repetitively. The idea is very simple, given
an input image, visual correspondences that are found by
the matching algorithm can be used as a new input to
further applications of the algorithm. This results in nding
an increasing number of visually similar images, thus, the
matching recall can be improved. To better understand the
idea refer to Fig. 4. An input image shown in Fig. 4a is
4We used a Java implementation of the algorithm using the standard
settings as described by [2]. The implementation is provided by Eugen
Labun and is available at:
http://homepages.thm.de/~elbn98/imagejsurf/
(a) Input image</p>
      <p>(b) A direct match
(c) Indirect matching images
visually matched based on the SURF descriptor with images
found in the same geographical location. One of the found
matchings is shown in Fig. 4b. Although other images (e.g.
Fig. 4c) are also visually similar to the input image they
have not been discovered by the matching algorithm. For
this purpose, a new visual matching is performed using the
image in Fig. 4b as input. This results in nding
additional matching images as can be seen in Fig. 4c. Those
images are, in turn, visually similar to the initial input
image. The introduced matching approach can be represented
in a tree structure5 as it is shown in Fig. 5. The root of
the tree corresponds to the original input image. Visually
similar images which are connected with a single edge are
considered as a direct match. The iterative matching
implies an increased computation which reaches its maximum
when a naive method is applied in which all the matching
images from a previous iteration are used as input for the
next iteration. To reduce the computation cost, input of
the next matching iterations have to be wisely selected. It
can be noticed that visual correspondences of an input
image can intersect in the sets of interest points which they
share with the input image. Therefore, it can be su cient
for the next matching iteration to only consider images that
match the input image according to di erent set of interest
points. To achieve that, we propose clustering the visual
correspondences of an input image based on the common
interest points which they share with the input image.
Consequently, only one representative image from each cluster
5This tree representation only holds if visual similarities among
images resulting from matching the input image are ignored. Otherwise,
the structure corresponds to a graph.
can be used as input for the next matching iteration (Fig.
5). Creating the clusters also implies comparing image
descriptors. However, the number of compared descriptors is
sharply reduced because only the descriptors of the interest
points that correspond to the input image are considered. In
general, this number corresponds to tens of interest points
and in most cases to less than ten, while the whole number
of interest points extracted from an image can exceed
thousands.</p>
      <p>The e ectiveness of the iterative image matching approach
and the trade o between using the naive and the clustering
approach is evaluated in section 4.1.</p>
    </sec>
    <sec id="5">
      <title>Candidate Word Importance</title>
      <p>The term P (wijIj ) can be interpreted as an indicator to
how mutually discriminative are an image and a word. We
directly derive the proxy value for this term form the model
seen in Fig. 3 using a simple frequency based approach as
follows:</p>
      <p>P (wijIj ) =
This measure assumes a uniform probability for all words
concerning a given image. This value increases when a
smaller number of tags are used to annotate the image. The
m
value of P P (wijIj ) can be considered as a weighted voting
j=1
for the word. Each vote for a word wi given by an image Ij
is scaled by the inverse frequency of the words annotating
that image.</p>
      <p>In this paper we aim to compare the above presented
measure to a TF-IDF (Term Frequency-Inverse Document
Frequency) based approach. In a similar manner to [14], an
adapted version of the TF-IDF measure can be used to
estimate the word probability while considering a set of images
annotated with it.</p>
      <p>Since we assume that each word can be used only once to
annotate a given image, one to one correspondence between
an image and a document results in the same term frequency
value for all tags annotating that image, i.e., one to the total
number of tags. To deal with this problem, a more realistic
measure for term frequency can be de ned as follows: let
R C be the set of visual neighbors of the input image and
Rw R the subset of visual neighbors which are annotated
with w. We consider Rw as one document and calculate the
term frequency of the word w as follows:
The inverse document frequency for w is calculated in a
traditional way. Assume that each image in C corresponds
to a document and let Cw C indicate the subset of images
in C which are annotated with w, the inverse document
frequency can be calculated as follows:
The term log2jCj is used for normalization.</p>
    </sec>
    <sec id="6">
      <title>User Related Issues</title>
      <p>Since tags are contributed from users, the importance of
a tag as a recommended annotation candidate can be
determined by its usage pattern by groups of users. Generally,
users tend to use the same set of words to annotate their
image collections even if the images share a little or no visual
similarity. This results in increasing the usage frequency of
some words in a way that does not re ect its importance
for automatic annotation. To address this problem, we
propose scaling the importance value of a candidate word by
a user's factor: U . In this respect, we propose two ways
to calculate U . One way is to take into consideration the
whole set of unique users who tagged images captured in a
given geographical area. In this case, U can be calculated as
the proportion of unique users who annotated images using
some word wi to the total number of unique users:
Uall users =
number of unique users using wi</p>
      <p>total number of unique users
On the other hand, U can be estimated by only considering
unique users who used wi. In this case, U can be given as:
number of unique users using wi</p>
      <p>Uusers of wi = total number of occurrences of wi
In the next sections we will show how the quality of the
recommended tag can vary according to the provided measures
for word importance as well as the user considerations.</p>
      <p>In this section di erent evaluation scenarios are presented.
In the rst part, we will show the e ect of an iterative
image matching on improving the matching recall. Hereby, we
compare two variations of the algorithm: a naive and a
clustering based approach. The evaluation reports the achieved
precision and recall and the runtime taken by each method.
The second part is concerned with evaluating the
annotation performance. For this purpose, we created our ground
truth and used a subset of the NUS-WIDE [3] dataset and
calculated the average precision and recall.
P (wijIj) =</p>
      <p>T F (w):IDF (w)</p>
      <p>Effectiveness of Iterative Image Matching
The ground truth for this test consists of groups of images
crawled from Flickr, Google Images and the European Cities
50K dataset [1]. Each group of the datasets contains images
which have the same contents but vary in scale, rotation, and
illumination. Moreover, the dataset expands over images of
di erent categories, such as outdoor, indoor, buildings,
nature etc. In total, we gathered 69 such groups with average
of 70 images each.</p>
      <p>From each group an image is selected and matched against
the remaining images of the same group. The decision about
the visual similarity between two images is done based on
the number of the interest points they share. It has been
proposed that image matching approaches which are based
on nding interest points correspondences provide a
precision of 1 by using 5 common interest points as a threshold
[7][10]. To exactly determine this threshold, we matched the
input images to a group of 2000 images covering di erent
categories. The results showed that a matching threshold
of 4 common interest points produced zero false positives
(precision of 1%) for almost all input images (the results are
not show due to space limitations).</p>
      <p>We adopted this matching threshold (4 interest points) and
used it evaluate the matching recall under di erent number
of iterations. In the iterative approach the matching
images are used as input for further iterative applications of
the same matching algorithm. The selection of the input
images for each matching iteration is done in two ways: a naive
one which considers all images found by a previous iteration,
and a clustering based method which uses one representative
image from each cluster as input. We used agglomerative
hierarchal clustering to build the clusters. Agglomerative
clustering is convenient to our case since we don't have to x the
number of the clusters and it is e cient since the number
of the images that will be clustered before each matching
iteration is small. After building the clusters, a random
selection of a representative image from each cluster is applied
to determine the inputs of the next matching iteration.</p>
      <p>Fig. 6a shows the average matching recall for 0
(corresponds to a single matching without iterations) to 4
iterations. It can be seen, that the iterative approach results
in signi cant recall improvement with an increasing number
of iterations. Furthermore, the experiments show that for
76% of the cases 3 iterations were enough to achieve the
maximum recall. On the other hand, the clustering based
approach shows slight drop in the average recall compared to
the naive approach, however, it provides better performance
(Fig. 6b). The average runtime of the clustering approach
is less than that of the naive approach and this di erence
increases by an increasing number of iterations. For
example, by four iterations the clustering based approach is more
than three times faster than the naive approach with a loss
of about 0.065 in the recall.</p>
    </sec>
    <sec id="7">
      <title>Quality of Recommended Tags</title>
      <p>The performance of the proposed annotation approach is
evaluated by a leave-one-out cross-validation (LOOCV)
approach: using a human-produced ground truth set of tagged
images we compare the tags automatically generated with
the actual ones. For this purpose, we created a dataset of
100 images and annotated them manually. Each image is
geotagged and described with 21 tags on average. For each
test image the annotation approach is applied and the
pre</p>
      <sec id="7-1">
        <title>1 Iteration</title>
      </sec>
      <sec id="7-2">
        <title>2 Iterations 3Iterations Avg Recall - Clustering 4Iterations</title>
      </sec>
      <sec id="7-3">
        <title>2 Iterations</title>
        <p>(a) Average recall at di erent matching iterations (b) Clustering-based vs. naive iterative matching:
Average matching runtime needed by each approach
at di erent matching iterations
0.5
llca0.4
e
R
0.3
cision and recall are calculated by comparing the produced
tags to the reference tags. After that, the average of
precision and recall are calculated for the total set of test images.</p>
        <p>To show the e ect of the produced ranking, the average
precision and recall are demonstrated at di erent annotation
lengths.</p>
        <p>Word Importance / User Related Factor
Weighted Voting/ - TF-IDF/
Weighted Voting/Uall users TF-IDF/Uall users</p>
        <p>Weighted Voting/Uusers of w TF-IDF/Uusers of w</p>
        <p>The annotation approach can be con gured according to
two parameters. First, the word importance which can be
calculated in two di erent ways: a weighted voting
(Equation 3) and TF-IDF based approach (Equation 6). The
second parameter is the user related factor U which can also
can be calculated in two ways (Equations 7 and 8). In
total, the two parameters can be combined in 6 di erent ways
(Table 1).</p>
        <p>Fig. 7 and 8 show that combination between the weighted</p>
      </sec>
      <sec id="7-4">
        <title>1 Iteration 0.5 0.4 0.3</title>
        <p>ll
caeR
0.2
voting and the user factor Uall users provides the best
recall and precision. This is a followed by TF-IDF combined
with the same user factor. Additionally, we evaluated the
annotation approach with a group of 88 images taken from
NUS-WIDE dataset. NUS-WIDE database contains about
50 thousands geotagged images. Each image in the database
is annotated with user tags in a uncontrolled manner, thus,
the provided tag are somehow noisy. We cleaned a subset
of the tags by removing irrelevant ones such numbers, stop
words and user names. Fig. 9 and 10 show how the
annotation performance using the weighted voting approach
without user consideration under 0 and 1 iteration. The
results show a clear improvement in the precision and recall
when the iterative approach is applied. This is due to the
fact, that the iterative matching approach results in nding
additional user-tagged images. Consequently, the size of the
candidate tags set as well as the accuracy of the produced
tag ranking increase.</p>
        <sec id="7-4-1">
          <title>5. CONCLUSIONS</title>
          <p>In this paper we presented an approach for automatic
image annotation. It exploits geotagged images contributed
to the web to automatically annotate new untagged
images captured in the same location. We showed how user
contributed images can be geographically ltered, visually
matched, and how their tags can be used to annotate new
images. The extracted information are combined in a
exible statistical model which is able to provide an image-word
relevance score. A proposal is made to improve the visual
matching by applying additional matching iterations and a
solution for improving the performance based on image
clustering is provided. Finally, the quality of the produced tags
is evaluated by a human created ground truth. In our future
work, we will consider performing further evaluation
scenarios on larger datasets. Additionally, we aim to extend the
probabilistic model to consider further image-word,
imageimage and word-word relations. We will also consider
enhancing the performance of the provided approach so that
it can t an online system.</p>
        </sec>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref>
        <mixed-citation>
          [1]
          <string-name>
            <given-names>Y.</given-names>
            <surname>Avrithis</surname>
          </string-name>
          ,
          <string-name>
            <given-names>G.</given-names>
            <surname>Tolias</surname>
          </string-name>
          ,
          <article-title>and</article-title>
          <string-name>
            <given-names>Y.</given-names>
            <surname>Kalantidis</surname>
          </string-name>
          .
          <article-title>Feature map hashing: Sub-linear indexing of appearance and global geometry</article-title>
          .
          <source>In in Proceedings of ACM Multimedia (Full paper) (MM</source>
          <year>2010</year>
          ), Firenze, Italy,
          <year>October 2010</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [2]
          <string-name>
            <given-names>H.</given-names>
            <surname>Bay</surname>
          </string-name>
          ,
          <string-name>
            <given-names>A.</given-names>
            <surname>Ess</surname>
          </string-name>
          ,
          <string-name>
            <given-names>T.</given-names>
            <surname>Tuytelaars</surname>
          </string-name>
          , and
          <string-name>
            <surname>L.</surname>
          </string-name>
          <article-title>Van Gool. Speeded-up robust features (surf)</article-title>
          .
          <source>Comput. Vis. Image Underst.</source>
          ,
          <volume>110</volume>
          :
          <fpage>346</fpage>
          {
          <fpage>359</fpage>
          ,
          <year>June 2008</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [3]
          <string-name>
            <given-names>T.-S.</given-names>
            <surname>Chua</surname>
          </string-name>
          ,
          <string-name>
            <given-names>J.</given-names>
            <surname>Tang</surname>
          </string-name>
          ,
          <string-name>
            <given-names>R.</given-names>
            <surname>Hong</surname>
          </string-name>
          ,
          <string-name>
            <given-names>H.</given-names>
            <surname>Li</surname>
          </string-name>
          ,
          <string-name>
            <given-names>Z.</given-names>
            <surname>Luo</surname>
          </string-name>
          ,
          <article-title>and</article-title>
          <string-name>
            <given-names>Y.-T.</given-names>
            <surname>Zheng</surname>
          </string-name>
          .
          <article-title>Nus-wide: A real-world web image database from national university of singapore</article-title>
          .
          <source>In Proc. of ACM Conf. on Image and Video Retrieval (CIVR'09)</source>
          , Santorini, Greece.,
          <year>July</year>
          8-
          <fpage>10</fpage>
          ,
          <year>2009</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [4]
          <string-name>
            <given-names>R. L.</given-names>
            <surname>Cilibrasi</surname>
          </string-name>
          and
          <string-name>
            <given-names>P. M. B.</given-names>
            <surname>Vitanyi</surname>
          </string-name>
          .
          <article-title>The google similarity distance</article-title>
          .
          <source>IEEE Trans. on Knowl. and Data Eng.</source>
          ,
          <volume>19</volume>
          :
          <fpage>370</fpage>
          {
          <fpage>383</fpage>
          ,
          <year>March 2007</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [5]
          <string-name>
            <given-names>R.</given-names>
            <surname>Datta</surname>
          </string-name>
          ,
          <string-name>
            <given-names>D.</given-names>
            <surname>Joshi</surname>
          </string-name>
          ,
          <string-name>
            <given-names>J.</given-names>
            <surname>Li</surname>
          </string-name>
          , and
          <string-name>
            <given-names>J. Z.</given-names>
            <surname>Wang</surname>
          </string-name>
          .
          <article-title>Image retrieval: Ideas, in uences, and trends of the new age</article-title>
          . ACM Comput. Surv.,
          <volume>40</volume>
          :
          <issue>5</issue>
          :
          <issue>1</issue>
          {
          <issue>5</issue>
          :
          <fpage>60</fpage>
          ,
          <year>May 2008</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [6]
          <string-name>
            <given-names>J.</given-names>
            <surname>Eakins</surname>
          </string-name>
          ,
          <string-name>
            <given-names>M.</given-names>
            <surname>Graham</surname>
          </string-name>
          , and
          <string-name>
            <given-names>J. I. S. C.</given-names>
            <surname>T</surname>
          </string-name>
          . A.
          <string-name>
            <surname>Programme</surname>
          </string-name>
          .
          <source>Content-based image retrieval</source>
          ,
          <year>1999</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [7]
          <string-name>
            <given-names>G. J. F.</given-names>
            <surname>Jones</surname>
          </string-name>
          ,
          <string-name>
            <given-names>D.</given-names>
            <surname>Byrne</surname>
          </string-name>
          ,
          <string-name>
            <given-names>M.</given-names>
            <surname>Hughes</surname>
          </string-name>
          ,
          <string-name>
            <given-names>N. E.</given-names>
            <surname>O'Connor</surname>
          </string-name>
          , and
          <string-name>
            <given-names>A.</given-names>
            <surname>Salway</surname>
          </string-name>
          .
          <article-title>Automated annotation of landmark images using community contributed datasets and web resources</article-title>
          . In T. Declerck,
          <string-name>
            <given-names>M.</given-names>
            <surname>Granitzer</surname>
          </string-name>
          ,
          <string-name>
            <given-names>M.</given-names>
            <surname>Grzegorzek</surname>
          </string-name>
          ,
          <string-name>
            <given-names>M.</given-names>
            <surname>Romanelli</surname>
          </string-name>
          ,
          <string-name>
            <given-names>S. M.</given-names>
            <surname>Ru</surname>
          </string-name>
          ger, and M. Sintek, editors,
          <source>SAMT</source>
          , volume
          <volume>6725</volume>
          <source>of Lecture Notes in Computer Science</source>
          , pages
          <fpage>111</fpage>
          {
          <fpage>126</fpage>
          . Springer,
          <year>2010</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [8]
          <string-name>
            <given-names>L.</given-names>
            <surname>Juan</surname>
          </string-name>
          and
          <string-name>
            <given-names>O.</given-names>
            <surname>Gwun</surname>
          </string-name>
          .
          <article-title>A comparison of sift, pca-sift and surf</article-title>
          .
          <source>International Journal of Image Processing (IJIP)</source>
          ,
          <volume>3</volume>
          (
          <issue>5</issue>
          ),
          <year>2010</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [9]
          <string-name>
            <given-names>Y.</given-names>
            <surname>Ke</surname>
          </string-name>
          and
          <string-name>
            <given-names>R.</given-names>
            <surname>Sukthankar</surname>
          </string-name>
          .
          <article-title>Pca-sift: A more distinctive representation for local image descriptors</article-title>
          .
          <source>Computer Vision and Pattern Recognition</source>
          ,
          <source>IEEE Computer Society Conference on</source>
          ,
          <volume>2</volume>
          :
          <fpage>506</fpage>
          {
          <fpage>513</fpage>
          ,
          <year>2004</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [10]
          <string-name>
            <given-names>Y.</given-names>
            <surname>Ke</surname>
          </string-name>
          ,
          <string-name>
            <given-names>R.</given-names>
            <surname>Sukthankar</surname>
          </string-name>
          , and
          <string-name>
            <given-names>L.</given-names>
            <surname>Huston</surname>
          </string-name>
          .
          <article-title>E cient near-duplicate detection and sub-image retrieval</article-title>
          .
          <source>In ACM Multimedia</source>
          , volume
          <volume>4</volume>
          ,
          <issue>page 5</issue>
          ,
          <year>2004</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [11]
          <string-name>
            <given-names>X.</given-names>
            <surname>Li</surname>
          </string-name>
          ,
          <string-name>
            <given-names>C. G. M.</given-names>
            <surname>Snoek</surname>
          </string-name>
          , and
          <string-name>
            <given-names>M.</given-names>
            <surname>Worring</surname>
          </string-name>
          .
          <article-title>Learning social tag relevance by neighbor voting</article-title>
          .
          <source>IEEE Transactions on Multimedia</source>
          ,
          <volume>11</volume>
          (
          <issue>7</issue>
          ):
          <fpage>1310</fpage>
          {
          <fpage>1322</fpage>
          ,
          <year>2009</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [12]
          <string-name>
            <given-names>J.</given-names>
            <surname>Liu</surname>
          </string-name>
          ,
          <string-name>
            <given-names>B.</given-names>
            <surname>Wang</surname>
          </string-name>
          ,
          <string-name>
            <given-names>M.</given-names>
            <surname>Li</surname>
          </string-name>
          ,
          <string-name>
            <given-names>Z.</given-names>
            <surname>Li</surname>
          </string-name>
          ,
          <string-name>
            <given-names>W.</given-names>
            <surname>Ma</surname>
          </string-name>
          ,
          <string-name>
            <given-names>H.</given-names>
            <surname>Lu</surname>
          </string-name>
          , and
          <string-name>
            <given-names>S.</given-names>
            <surname>Ma</surname>
          </string-name>
          .
          <article-title>Dual cross-media relevance model for image annotation</article-title>
          .
          <source>In Proceedings of the 15th international conference on Multimedia, MULTIMEDIA '07</source>
          , pages
          <fpage>605</fpage>
          {
          <fpage>614</fpage>
          , New York, NY, USA,
          <year>2007</year>
          . ACM.
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [13]
          <string-name>
            <given-names>D. G.</given-names>
            <surname>Lowe</surname>
          </string-name>
          .
          <article-title>Distinctive image features from scale-invariant keypoints</article-title>
          .
          <source>Int. J. Comput. Vision</source>
          ,
          <volume>60</volume>
          :
          <fpage>91</fpage>
          {
          <fpage>110</fpage>
          ,
          <year>November 2004</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [14]
          <string-name>
            <given-names>M.</given-names>
            <surname>Naaman</surname>
          </string-name>
          ,
          <string-name>
            <given-names>S.</given-names>
            <surname>Ahern</surname>
          </string-name>
          ,
          <string-name>
            <given-names>R.</given-names>
            <surname>Nair</surname>
          </string-name>
          , and
          <string-name>
            <given-names>T.</given-names>
            <surname>Rattenbury</surname>
          </string-name>
          .
          <article-title>How ickr helps us make sense of the world: context and content in community-contributed media collections</article-title>
          .
          <source>In In Proceedings of the 15th International Conference on Multimedia (MM2007</source>
          , pages
          <fpage>631</fpage>
          {
          <fpage>640</fpage>
          . ACM,
          <year>2007</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [15]
          <string-name>
            <given-names>G.</given-names>
            <surname>Qiu</surname>
          </string-name>
          .
          <article-title>Image coding using a coloured pattern appearance model</article-title>
          .
          <source>In Visual Communication and Image Processing</source>
          ,
          <year>2001</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [16]
          <string-name>
            <given-names>T.</given-names>
            <surname>Tachibanaya</surname>
          </string-name>
          .
          <article-title>Description of exif le format</article-title>
          . URL http://park2. wakwak. com/tsuruzoh/Computer/Digicams/exif-e. html.
          <source>February</source>
          ,
          <year>2001</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [17]
          <string-name>
            <given-names>L.</given-names>
            <surname>Wu</surname>
          </string-name>
          ,
          <string-name>
            <given-names>L.</given-names>
            <surname>Yang</surname>
          </string-name>
          ,
          <string-name>
            <given-names>N.</given-names>
            <surname>Yu</surname>
          </string-name>
          , and
          <string-name>
            <given-names>X.-S.</given-names>
            <surname>Hua</surname>
          </string-name>
          .
          <article-title>Learning to tag</article-title>
          . In 18th International
          <source>World Wide Web Conference</source>
          , pages
          <fpage>361</fpage>
          {
          <fpage>361</fpage>
          ,
          <year>April 2009</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          [18]
          <string-name>
            <given-names>N.</given-names>
            <surname>Zhou</surname>
          </string-name>
          ,
          <string-name>
            <given-names>W. K.</given-names>
            <surname>Cheung</surname>
          </string-name>
          ,
          <string-name>
            <given-names>G.</given-names>
            <surname>Qiu</surname>
          </string-name>
          , and
          <string-name>
            <given-names>X.</given-names>
            <surname>Xue</surname>
          </string-name>
          .
          <article-title>A hybrid probabilistic model for uni ed collaborative and content-based image tagging</article-title>
          .
          <source>IEEE Trans. Pattern Anal. Mach. Intell.</source>
          ,
          <volume>33</volume>
          :
          <fpage>1281</fpage>
          {
          <fpage>1294</fpage>
          ,
          <year>July 2011</year>
          .
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>