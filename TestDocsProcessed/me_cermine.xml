<article>
  <front>
    <journal-meta />
    <article-meta>
      <title-group>
        <article-title>Tag Similarity in Folksonomies</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author">
          <string-name>Hatem Mousselly-Sergieh</string-name>
          <email>hatem.mousselly-sergieh@insa-lyon.fr</email>
          <xref ref-type="aff" rid="0">0</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Elöd Egyed-Zsigmond</string-name>
          <email>elod.egyed-zsigmond@insa-lyon.fr</email>
          <xref ref-type="aff" rid="0">0</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Mario Döller</string-name>
          <email>mario.doeller@fh-kufstein.ac.at</email>
          <xref ref-type="aff" rid="0">0</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Gabriele Gianini</string-name>
          <email>gabriele.gianini@unimi.it</email>
          <xref ref-type="aff" rid="0">0</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Harald Kosch</string-name>
          <email>harald.kosch@unipassau.de</email>
          <xref ref-type="aff" rid="0">0</xref>
        </contrib>
        <contrib contrib-type="author">
          <string-name>Jean-Marie Pinon</string-name>
          <email>jean-marie.pinon@insalyon.fr</email>
          <xref ref-type="aff" rid="0">0</xref>
        </contrib>
        <aff id="0">
          <label>0</label>
          <institution>Xu X., Yuruk N., Feng Z., Schweiger T. A., “Scan: a structural clustering algorithm for net- works”, Proceedings of the 13th ACM SIGKDD international conference on Knowledge discovery and data mining, ACM</institution>
          ,
          <addr-line>2007, p. 824-833</addr-line>
        </aff>
      </contrib-group>
      <abstract>
        <p>Folksonomies - collections of user-contributed tags, proved to be efficient in reducing the inherent semantic gap. However, user tags are noisy; thus, they need to be processed before they can be used by further applications. In this paper, we propose an approach for bootstrapping semantics from folksonomy tags. Our goal is to automatically identify semantically related tags. The approach is based on creating probability distribution for each tag based on co-occurrence statistics. Subsequently, the similarity between two tags is determined by the distance between their corresponding probability distributions. For this purpose, we propose an extension for the well-known Jensen-Shannon Divergence. We compared our approach to a widely used method for identifying similar tags based on the cosine measure. The evaluation shows promising results and emphasizes the advantage of our approach.</p>
      </abstract>
      <kwd-group>
        <kwd>Folksonomies</kwd>
        <kwd>Tag Similarity</kwd>
        <kwd>Tag Clustering</kwd>
        <kwd>Semantic Web</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="1">
      <title>-</title>
      <p>RÉSUMÉ. Les folksonomies sont des collections d’annotations créés de manière collaborative par
plusieurs utilisateurs. Afin d’améliorer la recherche d’information à l’aide de folksonomies, leur
classification permet d’apporter des solutions à certains des problèmes inhérentes au caractère
libre et collaboratif des folksonomies. Ces problèmes sont les fautes de frappe, des
séparationscollage d’expressions, le multilinguisme, etc. Dans ce papier nous proposons une nouvelle
méthode de classification d’annotations basé sur la prise en compte de la cooccurrence des mots
avec un ensemble de mots fréquents. Nous utilisation la distribution de cooccurrences des mots
peu fréquents avec l’ensemble de mots fréquents afin de créer une fonction de similarité entre
les mots. L’approche a été évaluée sur des ensembles d’annotations provenant de Flickr
associés à des images prises dans quelques grandes villes. Nous avons comparé notre méthode à un
algorithme plus classique basé sur la distance de cosinus entre des vecteurs de mots.</p>
      <sec id="1-1">
        <title>1. Introduction</title>
        <p>With the emergence of Web 2.0, users become able to add contents to the web by
themselves. To alleviate the semantic gap when retrieving web resources tagging (Voss,
2007) was proposed as a simple and efficient solution. Tagging allows users to
annotate any kind of web resources, such as images, video, text, etc. with keywords called
"tags". A collection of tags that are created by different users is called a folksonomy
(Vanderwal, 2010). Currently, many web portals (e.g. Flickr 1, delicious 2) allow users
to tag their resources as well as to collaborate with other users to perform the
annotation task. Folksonomies have been used with success by different applications. They
enable the identification of user communities and allow efficient search and browsing
(Diederich et Iofciu, 2006) (Hotho et al., 2006). Additionally, many recommendation
system benefit from the user-generated tags in creating their recommendations (Sergieh
et al., 2012) (Li et al., 2009)</p>
        <p>Tags can be easily created since they do not require any special knowledge and
user can freely use whatever they think suitable to describe web resources. However,
this freedom poses a real challenge for applications that use folksonomies. Tags are
ambiguous (e.g. use of different synonyms), they can be in different languages (e.g.
English: Bridge and German: Brücke ) and contain inflections (e.g. book vs. books).
Additionally, tags which consist of word combinations are expressed differently. Some
users use the space characters (e.g. Eiffel tower), other users use hyphens or underscores
to indicate that the words represent a single unit (e.g. Eiffel-tower); while another group
of users may connect all the words together (e.g. Eiffeltower).</p>
        <p>In recent years, research suggested that using clustering techniques enables the
discovery of hidden structures in folksonomies and grouping related tags of different users
together (Begelman et al., 2006) (Papadopoulos et al., 2010) (Specia et Motta, 2007).
The identified tag clusters showed to be helpful for the generation of topic hierarchies
in tagging system and for improving content retrieval and browsing (Begelman et al.,
2006) (Brooks et Montanez, 2006).</p>
        <p>In line with that, we propose a novel approach for identifying similar tags in
folksonomies (Figure 1). First, a tag subset G is created from the most occurring tags in
the folksonomy. Next, for each tag g 2 G a co-occurrence probability distribution is
created. This is done as follows: First, we count the number of times in which g was
used with each of the other tags in G to annotate the same resources in the folksonomy.
Second, the co-occurrence counts of g with the other tags are converted to a probability
distribution by normalizing over the total occurrence of g in the folksonomy. The most
frequent tags are then clustered according to the distance between their corresponding
probability distributions.</p>
        <p>In a further step, probability distributions are calculated for the less frequent tags (The
set W in Figure 1). Here, the probability distributions are computed based on the
cooccurrences of the tag with each of the frequent tag clusters (instead of individual tags)
1. http://www.flickr.com/
2. http://www.delicious.com/
which were generated in the previous step.</p>
        <p>To find similar tags, the distances between the co-concurrence probability distributions
must be calculated. For this purpose, we extend the well-known probability
similarity measure Jensen-Shannon Divergence (JSD)(Manning et Schütze, 1999) so that it
can take into account the sample size from which the co-occurrence distributions are
calculated.</p>
        <p>Calculate tag co-occurrences
Calculate distribution
distances (JSD/AJSD)
Calculate tag co-occurrence
with the tag clusters C
Calculate distribution
distances (JSD/AJSD)
Figure 1: The different steps followed by our approach for identifying similar tags
in folksonomies. JSD correspond to the Jensen-Shannon Divergence and AJSD
correspond to our extension of that measure</p>
        <p>We evaluated the proposed approach based on real datasets obtained from Flickr.
For this purpose, we crawled Flickr tags of images taken in four cities: London, San
Francisco, Munich and Venice. We compared our approach to a common solution that
identifies similar tags by calculating the cosine similarity between their co-occurrence
vectors. The results of the preliminary evaluation are promising. They show that our
approach outperforms the traditional approach.</p>
        <p>The rest of the paper is organized as follows: in the next section we review related
work. In section 3 the proposed tags clustering approach is discussed in detail. Section
4 shows preliminary evaluation results. Finally, we conclude and discuss future work in
section 5.</p>
      </sec>
      <sec id="1-2">
        <title>2. Related work</title>
        <p>Disregarding the application domain, works on tag clustering use in common the
graph of tag co-occurrences as input. In the graph, tags are represented as nodes while
an edge between two nodes reflects the similarity of the corresponding tags.
Furthermore, several similarity measures, which are mainly derived from tag co-occurrence
statistics, were proposed in the literature to assign weights to the edges. It is also
common to apply a clustering method, such as agglomerative clustering to generate the final
tag clusters.</p>
        <p>Tag clustering in (Begelman et al., 2006) is based on tag co-occurrence counts. For
each pair of tags, their co-occurrence according to the same resource is counted. A
cut-off threshold is then used to determine if the co-occurrence of the tags indicate
similarity. The cut-off threshold is determined using the first and the second derivatives of
the tag co-occurrence curve. However, no clear justification for that choice was given.
To determine the final clusters the tag co-occurrence matrix is fed to spectral bisection
clustering algorithm.</p>
        <p>In (Gemmell et al., 2008b) (Gemmell et al., 2008a) the authors use agglomerative
clustering to generate tag groups with a similarity measure based on the term
frequencyinverse document frequency (TF.IDF) (Baeza Yates et al., 1999). The generated tag
clusters are then used as a nexus between users and their interests.</p>
        <p>In (Specia et Motta, 2007) the authors propose a tag clustering approach based on
cooccurrence similarity between the tags. First, the tags are organized in a co-occurrence
matrix with the columns and the rows corresponding to the tags. The entries of the
matrix represent the number of times two tags were used together to annotate the same
resource. Each tag is represented by a co-occurrence vector and the similarity between
two tags is calculated by applying the cosine measure on the corresponding vectors. A
clustering algorithm is then defined over the co-occurrence similarity matrix.
In the work presented in (Simpson, 2008) a normalized tag co-occurrence based on
Jaccard measure was initially used to generate a tag graph. After that, tag clusters were
generated by applying iterative divisive clustering on the tag graph.</p>
        <p>Another algorithm for tag clustering which is based on the notion of ( ; ) cores
(Xu et al., 2007) was introduced in (Papadopoulos et al., 2010). For this purpose, a tag
graph is built and the edges were weighted according to structural similarity between
the nodes. In other words, tags that have a large number of common neighbors to each
other are grouped together.</p>
        <p>The focus of the introduced works is on finding a clustering approach that provides
best grouping of tags. They share the common idea of using tag similarity measures
that are based on co-occurrence of all tag pairs and they differ according to the applied
clustering algorithm.</p>
        <p>The work presented here makes a different hypothesis. We assume that providing a
good tag similarity/distance measure leads to a better clustering results disregarding the
applied clustering algorithm. Therefore, we provide a similarity/distance measure that
uses tag co-occurrence more efficiently. This is done by limiting the calculation to the
co-occurrence of each tag with a smaller subset of the most frequent tags (instead of
considering the co-occurrence with every tag in the folksonomy). Furthermore, we
provide a similarity measure that is aware of the fluctuations in the co-occurrence counts
caused according to the sampling. Briefly, the paper provides the following
contribu</p>
        <p>– A new tag similarity/distance measure based on the distance between
cooccurrence probability distributions of the tags.</p>
        <p>– An extension for calculating Jensen-Shannon divergence between two probability
distributions. The new measure deals with the inherent fluctuations of the estimated
distributions.</p>
        <p>– Evaluation and comparison of our method to the traditional tag co-occurrence
similarity measure.</p>
      </sec>
      <sec id="1-3">
        <title>3. Tag Clustering Approach</title>
        <sec id="1-3-1">
          <title>3.1. Introduction</title>
          <p>Traditionally, a folksonomy F can be defined as a tuple F = fT; U; R; Ag where
T is the set of tags that are contributed by a set of users U to annotate a set of
resources R. Two tags t1; t2 2 T occur together "co-occur" if they are used by one
or more users to describe a resource r 2 R. This is captured by the assignment set,
A = (u; t; r) 2 U T R.</p>
          <p>Tag co-occurrence form the initial input for the majority of works that aims at
identifying semantic similarity among tags. In our approach, we defined the following process
to identify similar tags in folksonomies:
1) We identify the collection of tags that occur most in the folksonomy.
2) We derive a probability distribution for each tag in the folksonomy based on their
co-occurrence with the most frequent tags.</p>
          <p>3) After the co-coccurrence proabilty distributions have been aquired, we calculate
the distance between them for each tag pair.</p>
          <p>4) Finally, two tags are considered similar if the distance between their distributions
is under a certain threshold.</p>
          <p>Figure 2 shows an example of the co-occurrence distribution of a subset of tags which
are used to annotate photos taken in the city of London (more details are provided in
the next section). The x-axis corresponds to the subset of most frequent tags. The y-axis
shows the co-occurrence distribution of four less frequent tags, namely: "big", "ben",
"river" and "thames". It can be seen that the tags "big" and "ben" show a similar
cooccurrence behavior. The same holds for "river" and "thames".</p>
          <p>Since the set of frequent tags can be correlated, the occurrence of a given tag with
one of the correlated frequent tags implies the co-occurrence with the other ones. To
consider this case, we extend the above process by first grouping the most frequent tags
according to the corresponding co-occurrence probability distributions. Now, instead
of using the individual frequent tags, the co-occurrence probability distributions of the</p>
        </sec>
      </sec>
    </sec>
    <sec id="2">
      <title>Frequent Terms</title>
      <p>Figure 2: Co-occurrence probability distribution (y-axis) of tags that are used to
annotate images in London over a subset of frequent tags (x-axis)
iliy0.5
t
b
ab0.4
o
r
P
e0.3
c
n
e
r
ru0.2
c
c
o
-o0.1
C
0.6
y
t
i
il0.5
b
a
b
ro0.4
P
e
c
n0.3
e
r
r
u
cc0.2
o
o
C0.1
less frequent tags are calculated over the clusters of frequent tags. Figure 3 illustrates
the co-occurrence probability over frequent tag clusters.</p>
      <sec id="2-1">
        <title>3.2. Formal Definitions</title>
        <p>– the subset of the most frequent tags G
higher than a predefined threshold) and
– the subset of the remainder tags W = T n G.</p>
        <p>T (tags with number of occurrences
Hereafter we indicate the most frequent tags by the variable g and the tags in the
complement set by w (i.e. g 2 G and w 2 W , whereas a tag, taken from either set will be
indicated by t 2 T ).</p>
        <p>Now we define the four steps for this first phase of data processing (the most
frequent tags processing), namely: co-occurrence measure, empirical probability
definition, tag dissimilarity computation and tag clustering:</p>
        <p>1) For a tag t 2 T we can quantify the co-occurrences with each of the most frequent
tags g 2 G, by counting the number of times #(t; g) in which t was used together with
g to annotate a resource. We can use this set of counts to create an histogram in the
variable g.</p>
        <p>2) Then, normalizing this histogram with the total number of co-occurrences of
t with the elements of the set G we obtain the empirical co-occurrence probability
distribution Pt(g) for the tag t with the elements g 2 G:</p>
        <p>Pt(g) = Pg2G #(t; g)
In view of what follows we can consider #(t; g) as a shorthand for #ttaagg(t; g), which
indicates the number of co-occurrences of a tag t with another tag g.</p>
        <p>3) Given the empirical co-occurrence probability distribution Pt1 (g) relative to a tag
t1 2 T and the co-occurrence probability distribution Pt2 (g) relative to a different tag
t2 2 T it is possible to define a dissimilarity metrics DAJSD(t1; t2) between the two
tags, based on information-theoretic divergences, namely an adaptation of the
JensenShannon Divergence (AJSD): due to the complexity of this step the definition and the
discussion are provided in the next section.</p>
        <p>4) This dissimilarity metrics between tags can then be used to classify them into
clusters. Indeed we applied this procedure to the elements of the set G of most
frequent tags itself: for every pair of tags g1; g2 2 G we computed the empirical
probability distributions Pg1 (g) and Pg2 (g) of co-occurrence with the other most frequent
tags, according to the above definition (1) and then computed the AJSD dissimilarity
metrics DAJSD(g1; g2), defined in the following section; finally we applied a
clustering algorithm over the set G using d as the dissimilarity metrics and achieved the
partitioning of the set G into clusters – in other words, we obtained from G a family
of mutually exclusive subsets G = fG1; :::; Gj ; Gk; :::Gng whose union covers G (i.e.</p>
      </sec>
    </sec>
    <sec id="3">
      <title>Gj T Gk = ; 8Gj ; Gk G and Sk Gk = G).</title>
      <p>At this point, as outlined in the previous section, we enter a second data processing
phase: we considered the less frequent tags w 2 W and repeated the co-occurrence
probability estimate, the dissimilarity calculation and finally the clustering procedure
– however we do this adopting an important variant, relating to the definition of
cooccurrence and described hereafter – so as to achieve a partitioning of W into a family
of subsets: W = fW1; W2; : : : ; Wmg.</p>
      <p>1) This time we did not relate a tag t to another tag, say g, but, rather, a tag t
to a cluster of tags Gk, i.e. the co-occurrence definition this time concerned the
cooccurrence of a tag and a cluster. There are several possible choices for providing a
measure of the co-occurrence of a tag and a cluster: we have chosen to use the number
of co-occurrences with the term which has the maximum number of co-occurrences.
Formally we defined the measure #tcalugs(w; Gk) of the co-occurrence between a tag w
and a cluster Gk as
#tcalugs(w; Gk) = max(#ttaagg(w; g))</p>
      <p>g2Gk
2) Consequently the empirical distribution of co-occurrences turned out to be
defined by</p>
      <p>Pw(Gk) = P
3) Based on the empirical distributions for all the pairs of terms wi; wj 2 W we
computed the AJSD dissimilarities DAJSD(wi; wj ), defined in the next section, and
4) were able to cluster the less frequent terms into clusters reflecting their
cooccurrence behaviour with respect to the high frequency term clusters.</p>
      <sec id="3-1">
        <title>3.3. Distance Measures</title>
        <p>We calculate the semantic similarity between two tags t1; t2 2 T based on the
similarity between their corresponding empirical co-occurrence probability distributions.</p>
        <p>In the literature, different method were used to calculate this distance (Cha, 2007)
(Huang, 2008) (Matsuo et Ishizuka, 2004). The Jensen-Shannon Divergence (JSD)
(Manning et Schütze, 1999) has shown to outperform other measures (Ljubešic´ et al.,
2008), including the the Kullbak-Leiber divergence, on which it is based. We use a
metrics derived from the Jensen-Shannon divergence and able to take into account the
statistical fluctuations due to the finitness of the sample: the rationale for using this
specific definition and the derivation of our metrics are developed hereafter.</p>
        <p>Consider the empirical co-occurrence probability distribution Pt1 (g), with g 2 G,
relative to a tag t1 and a set of high frequency tags G as defined in (1), and consider
the analogous empirical probability distribution Pt2 (g), with g 2 G, relative to a tag
t2 and the same set of high frequency tags G. For sake of denotational simplicity we
will indicate one value of the first distribution by P (g) and a value of the second by
Q(g), whereas we will use P for the first distribution as a whole and Q for the second
distribution as a whole.</p>
        <p>The most typical metrics for dissimilarity between two probability distributions is
the Kullbak-Leiber divergence DKL
DJS (P; Q)</p>
        <p>(DKL(P jjM ) + DKL(QjjM ))
The expression DKL(P jjQ) is not symmetric in P and Q but can be symmetrised as
follows:
DKL(P; Q)</p>
        <p>(DKL(P jjQ) + DKL(QjjP ))
Unfortunately this quantity becomes infinite as soon as either P or Q become null in
one point of the support set, due to the denominators in the logarithm arguments of the
two terms. In order to avoid this issue the denominators are substituted by M (g)
(P (g) + Q(g))=2 giving rise to the Jensen-Shannon divergence:
The value of this expression is not in general the maximum likelihood estimate of the
Jensen-Shannon divergence, mainly due to the unequal variances of the terms in the
sum. In order to find the maximum likelihood estimate d^ of the divergence we need to
proceed through error propagation, starting (a) from the Maximum Likelihood (ML)
estimate of P (g) and Q(g) based on the variables xg and yg, (b) considering and
propagating their statistical errors to the summation to which they participate and finally (c)
proceeding to term weighting according to the term uncertainty.</p>
        <p>(a) Thanks to the normality condition stated above, the ML estimates of the
probabilities P (g) and Q(g) are the following: the ML estimate of P (g) is xg = kg=n with
If, as in our case, the probabilities P and Q are not available, but only an estimate of
them is available through a finite sample represented in the form of an histogram for P
and an histogram for Q, then the divergence computed on the histograms is a random
variable; this variable, under appropriate assumptions, can be used to compute an
estimate of the divergence between P and Q using error propagation under a Maximum
Likelihood approach, as illustrated hereafter.</p>
        <p>Consider the channel on the value g of the histogram for P and for Q, characterized
respectively by the number of counts kg and hg, and define the following measured
frequencies
kg=n
hg=m
where n = Pg kg and m = Pg hg are the total counts for the first and second
histogram respectively. Then, for n high enough, the quantities xg and yg are normally
distributed around the true probabilities P (g) and Q(g) respectively. As a consequence
the measured Jensen-Shannon divergence d then is a stochastic variable, function of
those normal variables according to the following expression
d =</p>
        <sec id="3-1-1">
          <title>1 X</title>
          <p>variance given in first approximation by P2 (g) = kg=n2; the ML estimate of Q(g) is
yg = hg=m with variance given in first approximation by Q2(g) = hg=m2
(b) Consider the individual addendum term in the sum expression (6).
If the two variables xg and yg are independent, the variance propagation at the first
order is
Now substituting in this formula the above expressions for the variables and their
variances one gets the estimated variance 2(zg) of the term (7).</p>
          <p>(c) Define the (statistical) precision wg (to be used later as a weight) as follows:</p>
          <sec id="3-1-1-1">
            <title>4. Evaluation</title>
          </sec>
        </sec>
      </sec>
      <sec id="3-2">
        <title>4.1. Dataset</title>
        <p>The presented tag similarity approach was evaluated using folksonomies obtained
from Flickr. For this purpose, we used Flickr API to collect tags of images taken in a
specific geographical location. We downloaded images and the associated tags for four
cities: London, San Francisco, Venice and Munich. To avoid the influence of redundant
tags caused by bulk tagging, we limited our datasets to a single image per user. The
The maximum likelihood estimate of the quantity (6) is given by the following weighted
sum
The corresponding variance is 2(d^) = 1=Pg wg.</p>
        <p>We used d^as adapted Jensen-Shannon Divergence. Notice that this adapted JSD, due to
the statistical fluctuations in the samples, gives in general values greater than zero even
when two samples are taken from the same distribution, i.e. even when the true
divergence is zero. However by weighting the terms according to their (statistical) precision
it provides a ranking for the terms, which is correlated to the true ranking in a stronger
way than the raw JSD.
number of the acquired images differs from one location to the other. Table 1 shows the
number of images, the total number of tags and the number of unique tags
corresponding to the four cities.</p>
        <p>The tags were then subjected to a light cleaning process to remove meaningless
tags, such as technical (typically camera specific EXIF tags) and Flickr-specific tags.
We didn’t apply stemming on the tags because we wanted to check if our approach is
able to handle syntactic variations. Furthermore, we removed tags that were used by
less than 5 users.</p>
      </sec>
      <sec id="3-3">
        <title>4.2. Experimental Setup</title>
        <p>For each location-based folksonomy, we calculated tag similarity according to our
approach using JSD and the proposed extension which we denote as adapted JSD
(AJSD). For our algorithm two parameters have to be set: the number of the most
frequent tags k and the distance threshold for clustering the most frequent terms th.
Good results were achieved by choosing the top k frequent tags that account to at least
30% of the total tag occurrences in the folksonomy. Clustering threshold th = 0:03 and
th = 0:003 were used for JSD and the AJSD clustering, respectively. Table 2 shows the
number of generated clusters per distance measure and per city-based folksonomy.</p>
        <p>Furthermore, we compared our method to a widely used method which identifies
tag similarity based on the cosine similarity between the co-occurrence vectors of the
corresponding tags (Specia et Motta, 2007) (Begelman et al., 2006). In that approach,
each tag is represented by a vector, the components of which correspond to the
individual tags of the folksonomy. The entries of the tag vector are the co-occurrence counts
of the tag pairs. Finally, the similarity of two tags is determined using the cosine of the
angle between the corresponding tag vectors. We call this method as COSTag.
Figure 4: Sample tag clusters from the London folksonomy. The clusters were obtained
from a cut-off of a hierarchical cluster that was created based on the AJSD distance
measure</p>
        <p>To compare the different tag similarity/distance measure, we generated similarity
matrices according to JSD our extension of it AJSD, as well as according to the COSTag
measure. After that, we fed the produced similarity matrices into an agglomerative
clustering process with complete link as a merge criterion. To generate the final clusters, we
used cut-off thresholds for the generated hierarchal clusters that fulfill two conditions:
1) avoid large clusters that contain more than 15 elements and 2) produce almost the
same number of clusters from each of the similarity matrices. Note that, the choice of
the clustering algorithm is not the focus of our work in this paper. The purpose of the
clustering here is to enable comparing different similarity measures.</p>
        <p>Figure 4 shows a hierarchal representation of tag clusters generated by applying
our algorithm with the AJSD measure on the London folksonomy. This example shows
three typical clusters having synonyms and complementary words together. In the
cluster: (gothic, cathedral, church, abbey, westminster), westminster abbey is a named
entity, cathedral and church are synonyms, while gothic is an adjective. The cluster,
nevertheless makes sense for a human reader. It is possibe to consider these words as having
a similarity &gt; 0 in a comparison function. In a situation where there is no available
perfect semantic resource, which is the case most of the time when dealing with generic
texts or images, such clusters can improve information retrieval.</p>
      </sec>
      <sec id="3-4">
        <title>4.3. Results</title>
        <p>The quality of the produced clusters was evaluated manually by a group of 10 users.
To reduce the labor of the manual verification, we limited the evaluation to clusters that
contain at least 3 tags. Each user received a list of automatically generated tag clusters
(Table 3) and verified if the tags in each cluster were related (semantically, syntactically,
translation in different languages, etc.).</p>
        <p>Finally, the clusters were rated according to three quality scales:
– Good: meaning that all the tags in the cluster are related.
– Neutral: if a subset of the tags in the cluster are related while other tags are not.
– Bad: if the cluster contains no or very few related tags.
The clusters are categorized in given quality scales if more than 50% of the polled
users agree on that scale. Figure 5 shows for each city folksonomy the percentage of
clusters classified according the proposed quality scales. We notice that the percentage
of clusters classified as good exceeds 73% for all four folksonomies when AJSD is used.
If we add the neutral ones, we achieve more than 90% good results. This confronts us in
our research of discovering relations among words, looking on their use. It can be seen
that the user subjective evaluation favors the tag clusters that were generated according
to our proposed distance measures: AJSD.</p>
        <sec id="3-4-1">
          <title>5. Conclusion</title>
          <p>The method described in this paper enables to make emerge word clusters that have
a meaning. This is confirmed by the manual evaluations we carried out. These clusters
help create word similarity measures that go beyond the purely syntax based ones
without using explicit semantic resources such as ontologies. Two words belonging to the
same cluster can be considered as having a positive similarity. This can improve
similarity measures not only for texts, but also for images having annotations. Our method
can improve the quality of user created annotations, where words are not separated,
contain errors and descriptions are subjective and diverse. The clusters enable to locate
synonyms and named entities.</p>
        </sec>
      </sec>
    </sec>
    <sec id="4">
      <title>(a) London Folksonomy</title>
      <p>(b) SF Folksonomy
80%
70%
60%
50%
40%
30%
20%
10%
0%
80%
70%
60%
50%
40%
30%
20%
10%
0%
90%
80%
70%
60%
50%
40%
30%
20%
10%
0%
(c) Munich Folksonomy</p>
    </sec>
    <sec id="5">
      <title>(d) Venice Folksonomy</title>
      <p>Figure 5: User evaluation for clusters generated by our proposal using JSD and AJSD
and the adversary approach COSTag</p>
      <sec id="5-1">
        <title>6. References</title>
      </sec>
    </sec>
  </body>
  <back>
    <ref-list>
      <ref>
        <mixed-citation>
          <string-name>
            <given-names>Baeza</given-names>
            <surname>Yates</surname>
          </string-name>
          <string-name>
            <given-names>R</given-names>
            .,
            <surname>Ribeiro Neto</surname>
          </string-name>
          <string-name>
            <surname>B.</surname>
          </string-name>
          , others,
          <source>Modern information retrieval</source>
          , vol.
          <volume>463</volume>
          , ACM press New York.,
          <year>1999</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <given-names>Begelman G.</given-names>
            ,
            <surname>Keller</surname>
          </string-name>
          <string-name>
            <given-names>P.</given-names>
            ,
            <surname>Smadja</surname>
          </string-name>
          <string-name>
            <surname>F.</surname>
          </string-name>
          , others, “
          <article-title>Automated tag clustering: Improving search and ex- ploration in the tag space”</article-title>
          ,
          <source>Collaborative Web Tagging Workshop at WWW2006</source>
          , Edinburgh, Scotland,
          <year>2006</year>
          , p.
          <fpage>15</fpage>
          –
          <lpage>33</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <given-names>Brooks C. H.</given-names>
            ,
            <surname>Montanez</surname>
          </string-name>
          <string-name>
            <surname>N.</surname>
          </string-name>
          , “
          <article-title>Improved annotation of the blogosphere via autotagging and hier- archical clustering”</article-title>
          ,
          <source>Proceedings of the 15th international conference on World Wide Web, ACM</source>
          ,
          <year>2006</year>
          , p.
          <fpage>625</fpage>
          –
          <lpage>632</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Cha S</surname>
          </string-name>
          .-H., “
          <source>Comprehensive Survey on Distance/Similarity Measures between Probability Den- sity Functions”</source>
          ,
          <source>International Journal of Mathematical Models and Methods in Applied Sciences, vol. 1</source>
          , num.
          <volume>4</volume>
          ,
          <year>2007</year>
          , p.
          <fpage>300</fpage>
          –
          <lpage>307</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <given-names>Diederich J.</given-names>
            ,
            <surname>Iofciu</surname>
          </string-name>
          <string-name>
            <surname>T.</surname>
          </string-name>
          , “
          <article-title>Finding communities of practice from user proﬁles based on folk- sonomies”, Proceedings of the 1st International Workshop on Building Technology Enhanced Learning solutions for Communities of Practice</article-title>
          , Citeseer,
          <year>2006</year>
          , p.
          <fpage>288</fpage>
          –
          <lpage>297</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <given-names>Gemmell J.</given-names>
            ,
            <surname>Shepitsen</surname>
          </string-name>
          <string-name>
            <given-names>A.</given-names>
            ,
            <surname>Mobasher</surname>
          </string-name>
          <string-name>
            <surname>B.</surname>
          </string-name>
          ,
          <string-name>
            <surname>Burke R</surname>
          </string-name>
          ., “
          <article-title>Personalization in folksonomies based on tag clustering”, Intelligent techniques for web personalization &amp; recommender systems</article-title>
          , vol.
          <volume>12</volume>
          ,
          <year>2008</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <given-names>Gemmell J.</given-names>
            ,
            <surname>Shepitsen</surname>
          </string-name>
          <string-name>
            <given-names>A.</given-names>
            ,
            <surname>Mobasher</surname>
          </string-name>
          <string-name>
            <surname>B.</surname>
          </string-name>
          ,
          <string-name>
            <surname>Burke R</surname>
          </string-name>
          ., “Personalizing Navigation in Folksonomies Using Hierarchical Tag Clustering”,
          <source>Proceedings of the 10th international conference on Data Warehousing and Knowledge</source>
          <publisher-name>Discovery, DaWaK</publisher-name>
          ’
          <fpage>08</fpage>
          , Berlin, Heidelberg,
          <year>2008</year>
          , Springer-Verlag, p.
          <fpage>196</fpage>
          –
          <lpage>205</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <given-names>Hotho A.</given-names>
            ,
            <surname>Jäschke</surname>
          </string-name>
          <string-name>
            <given-names>R.</given-names>
            ,
            <surname>Schmitz</surname>
          </string-name>
          <string-name>
            <given-names>C.</given-names>
            ,
            <surname>Stumme</surname>
          </string-name>
          <string-name>
            <surname>G.</surname>
          </string-name>
          , “
          <article-title>Information retrieval in folksonomies: search and ranking”</article-title>
          ,
          <source>Proceedings of the 3rd European conference on The Semantic Web: research and applications</source>
          , ESWC’
          <fpage>06</fpage>
          , Berlin, Heidelberg,
          <year>2006</year>
          , Springer-Verlag, p.
          <fpage>411</fpage>
          –
          <lpage>426</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Huang A.</surname>
          </string-name>
          , “
          <article-title>Similarity measures for text document clustering”</article-title>
          ,
          <source>Proceedings of the Sixth New Zealand Computer Science Research Student Conference (NZCSRSC2008)</source>
          , Christchurch, New Zealand,
          <year>2008</year>
          , p.
          <fpage>49</fpage>
          –
          <lpage>56</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Li</surname>
            <given-names>X.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Snoek</surname>
            <given-names>C.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Worring</surname>
            <given-names>M.</given-names>
          </string-name>
          ,
          <article-title>“Learning social tag relevance by neighbor voting”</article-title>
          , Multimedia,
          <source>IEEE Transactions on</source>
          , vol.
          <volume>11</volume>
          , num.
          <volume>7</volume>
          ,
          <year>2009</year>
          , p.
          <fpage>1310</fpage>
          –
          <lpage>1322</lpage>
          , IEEE.
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Ljubešic</surname>
            ´
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Boras</surname>
            <given-names>D.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Bakaric</surname>
            ´
            <given-names>N.</given-names>
          </string-name>
          ,
          <string-name>
            <surname>Njavro</surname>
            <given-names>J.</given-names>
          </string-name>
          , “
          <article-title>Comparing measures of semantic similarity”</article-title>
          ,
          <source>30th International Conference on Information Technology Interfaces, Cavtat</source>
          ,
          <year>2008</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <given-names>Manning C.</given-names>
            ,
            <surname>Schütze</surname>
          </string-name>
          <string-name>
            <surname>H.</surname>
          </string-name>
          ,
          <source>Foundations of statistical natural language processing</source>
          , MIT press,
          <year>1999</year>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <given-names>Matsuo Y.</given-names>
            ,
            <surname>Ishizuka</surname>
          </string-name>
          <string-name>
            <surname>M.</surname>
          </string-name>
          ,
          <article-title>“Keyword extraction from a single document using word co-occurrence statistical information”</article-title>
          ,
          <source>International Journal on Artiﬁcial Intelligence Tools</source>
          , vol.
          <volume>13</volume>
          , num.
          <volume>01</volume>
          ,
          <year>2004</year>
          , p.
          <fpage>157</fpage>
          –
          <lpage>169</lpage>
          , World Scientiﬁc.
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <given-names>Papadopoulos S.</given-names>
            ,
            <surname>Kompatsiaris</surname>
          </string-name>
          <string-name>
            <given-names>Y.</given-names>
            ,
            <surname>Vakali</surname>
          </string-name>
          <string-name>
            <surname>A.</surname>
          </string-name>
          , “
          <article-title>A graph-based clustering scheme for identi- fying related tags in folksonomies”</article-title>
          ,
          <source>Proceedings of the 12th international conference on Data warehousing and knowledge discovery</source>
          ,
          <source>DaWaK</source>
          ’
          <fpage>10</fpage>
          , Berlin, Heidelberg,
          <year>2010</year>
          , Springer- Verlag, p.
          <fpage>65</fpage>
          –
          <lpage>76</lpage>
          .
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Sergieh H. M.</surname>
          </string-name>
          ,
          <string-name>
            <given-names>Gianini G.</given-names>
            ,
            <surname>Döller</surname>
          </string-name>
          <string-name>
            <given-names>M.</given-names>
            ,
            <surname>Kosch</surname>
          </string-name>
          <string-name>
            <given-names>H.</given-names>
            ,
            <surname>Egyed-Zsigmond</surname>
          </string-name>
          <string-name>
            <given-names>E.</given-names>
            ,
            <surname>Pinon</surname>
          </string-name>
          <string-name>
            <surname>J.-M.</surname>
          </string-name>
          ,
          <article-title>“Geo-based automatic image annotation”</article-title>
          ,
          <source>Proceedings of the 2nd ACM International Conference on Multimedia Retrieval</source>
          , ACM,
          <year>2012</year>
          , Page 46.
        </mixed-citation>
      </ref>
      <ref>
        <mixed-citation>
          <string-name>
            <surname>Simpson E.</surname>
          </string-name>
          , “
          <article-title>Clustering Tags in Enterprise and Web Folksonomies”, HP Labs Techincal Re- ports</article-title>
          , ,
          <year>2008</year>
          .
        </mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>